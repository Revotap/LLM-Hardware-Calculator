<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>LLM Hardware Calculator</title>
  <style>
    :root {
      --bg: #0f1117;
      --surface: #1a1d27;
      --surface2: #232736;
      --border: #2e3348;
      --accent: #6c63ff;
      --accent2: #4ecdc4;
      --text: #e4e4e7;
      --text-dim: #9ca3af;
      --warn: #f59e0b;
      --danger: #ef4444;
      --success: #22c55e;
    }

    * { margin: 0; padding: 0; box-sizing: border-box; }

    body {
      font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;
      background: var(--bg);
      color: var(--text);
      min-height: 100vh;
    }

    header {
      text-align: center;
      padding: 2rem 1rem 1rem;
      border-bottom: 1px solid var(--border);
      background: linear-gradient(180deg, #181b25 0%, var(--bg) 100%);
      position: relative;
    }

    header h1 {
      font-size: 1.8rem;
      font-weight: 700;
      background: linear-gradient(135deg, var(--accent), var(--accent2));
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
    }

    header p { color: var(--text-dim); margin-top: 0.3rem; font-size: 0.95rem; }

    .lang-switch {
      position: absolute;
      top: 1rem;
      right: 1.5rem;
      display: flex;
      gap: 0;
      border: 1px solid var(--border);
      border-radius: 6px;
      overflow: hidden;
    }

    .lang-switch button {
      padding: 0.35rem 0.7rem;
      background: var(--surface2);
      border: none;
      color: var(--text-dim);
      font-size: 0.8rem;
      cursor: pointer;
      transition: all 0.2s;
    }

    .lang-switch button.active {
      background: var(--accent);
      color: #fff;
    }

    .lang-switch button:not(.active):hover {
      background: var(--surface);
      color: var(--text);
    }

    .container {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 2rem;
      max-width: 1400px;
      margin: 0 auto;
      padding: 2rem;
    }

    @media (max-width: 960px) {
      .container { grid-template-columns: 1fr; }
      .results { position: static !important; }
    }

    .panel {
      background: var(--surface);
      border: 1px solid var(--border);
      border-radius: 12px;
      padding: 1.5rem;
    }

    .panel h2 {
      font-size: 1.1rem;
      margin-bottom: 1.2rem;
      color: var(--accent2);
      display: flex;
      align-items: center;
      gap: 0.5rem;
    }

    .section { margin-bottom: 1.5rem; }

    .section-title {
      font-size: 0.8rem;
      text-transform: uppercase;
      letter-spacing: 0.08em;
      color: var(--text-dim);
      margin-bottom: 0.8rem;
      border-bottom: 1px solid var(--border);
      padding-bottom: 0.4rem;
    }

    .field { margin-bottom: 1rem; }

    .field label {
      display: flex;
      justify-content: space-between;
      align-items: baseline;
      font-size: 0.85rem;
      margin-bottom: 0.35rem;
      color: var(--text);
    }

    .field label .hint {
      font-size: 0.75rem;
      color: var(--text-dim);
    }

    .field input[type="range"] {
      width: 100%;
      accent-color: var(--accent);
      cursor: pointer;
    }

    .field input[type="number"],
    .field select {
      width: 100%;
      padding: 0.5rem 0.7rem;
      background: var(--surface2);
      border: 1px solid var(--border);
      border-radius: 6px;
      color: var(--text);
      font-size: 0.9rem;
      outline: none;
      transition: border-color 0.2s;
    }

    .field input[type="number"]:focus,
    .field select:focus {
      border-color: var(--accent);
    }

    .slider-row {
      display: flex;
      align-items: center;
      gap: 0.7rem;
    }

    .slider-row input[type="range"] { flex: 1; }

    .slider-row .val {
      min-width: 5rem;
      text-align: right;
      font-size: 0.9rem;
      font-variant-numeric: tabular-nums;
      color: var(--accent2);
      font-weight: 600;
    }

    .results { position: sticky; top: 2rem; }

    .result-card {
      background: var(--surface2);
      border: 1px solid var(--border);
      border-radius: 10px;
      padding: 1.2rem;
      margin-bottom: 1rem;
      transition: border-color 0.3s;
    }

    .result-card.highlight {
      border-color: var(--accent);
      box-shadow: 0 0 20px rgba(108, 99, 255, 0.1);
    }

    .result-card h3 {
      font-size: 0.8rem;
      text-transform: uppercase;
      letter-spacing: 0.06em;
      color: var(--text-dim);
      margin-bottom: 0.5rem;
    }

    .result-card .value {
      font-size: 2rem;
      font-weight: 700;
      font-variant-numeric: tabular-nums;
    }

    .result-card .value.accent { color: var(--accent); }
    .result-card .value.accent2 { color: var(--accent2); }
    .result-card .value.warn { color: var(--warn); }

    .result-card .sub {
      font-size: 0.8rem;
      color: var(--text-dim);
      margin-top: 0.3rem;
    }

    .breakdown { margin-top: 0.8rem; }

    .breakdown-row {
      display: flex;
      justify-content: space-between;
      padding: 0.4rem 0;
      font-size: 0.85rem;
      border-bottom: 1px solid var(--border);
    }

    .breakdown-row:last-child { border-bottom: none; }
    .breakdown-row .label { color: var(--text-dim); }
    .breakdown-row .val { font-weight: 600; font-variant-numeric: tabular-nums; }

    .gpu-rec { margin-top: 1rem; }

    .gpu-tag {
      display: inline-block;
      padding: 0.3rem 0.7rem;
      border-radius: 6px;
      font-size: 0.8rem;
      font-weight: 600;
      margin: 0.2rem 0.3rem 0.2rem 0;
    }

    .gpu-tag.green { background: rgba(34,197,94,0.15); color: var(--success); border: 1px solid rgba(34,197,94,0.3); }
    .gpu-tag.yellow { background: rgba(245,158,11,0.15); color: var(--warn); border: 1px solid rgba(245,158,11,0.3); }
    .gpu-tag.red { background: rgba(239,68,68,0.15); color: var(--danger); border: 1px solid rgba(239,68,68,0.3); }

    .presets {
      display: flex;
      flex-wrap: wrap;
      gap: 0.4rem;
      margin-bottom: 1.2rem;
    }

    .presets button {
      padding: 0.35rem 0.75rem;
      background: var(--surface2);
      border: 1px solid var(--border);
      border-radius: 6px;
      color: var(--text);
      font-size: 0.78rem;
      cursor: pointer;
      transition: all 0.2s;
    }

    .presets button:hover,
    .presets button:focus-visible {
      border-color: var(--accent);
      background: rgba(108, 99, 255, 0.1);
    }

    .presets button.active {
      border-color: var(--accent);
      background: rgba(108, 99, 255, 0.2);
      color: #fff;
    }

    .info-banner {
      background: rgba(108, 99, 255, 0.08);
      border: 1px solid rgba(108, 99, 255, 0.2);
      border-radius: 8px;
      padding: 0.8rem 1rem;
      font-size: 0.82rem;
      color: var(--text-dim);
      line-height: 1.5;
      margin-bottom: 1rem;
    }

    .warn-banner {
      background: rgba(245, 158, 11, 0.08);
      border: 1px solid rgba(245, 158, 11, 0.25);
      border-radius: 8px;
      padding: 0.6rem 0.9rem;
      font-size: 0.8rem;
      color: var(--warn);
      line-height: 1.4;
      margin-bottom: 0.8rem;
      display: none;
    }

    .multi-gpu-info {
      font-size: 0.82rem;
      color: var(--accent2);
      margin-top: 0.3rem;
    }

    .lora-section { display: none; }
    .lora-section.visible { display: block; }
  </style>
</head>
<body>

<header>
  <h1>LLM Hardware Calculator</h1>
  <p id="headerSub"></p>
  <div class="lang-switch">
    <button id="langEn" class="active" onclick="setLang('en')">EN</button>
    <button id="langDe" onclick="setLang('de')">DE</button>
  </div>
</header>

<div class="container">
  <!-- Input Panel -->
  <div class="panel">
    <h2 id="h2Config"></h2>

    <div class="presets" id="presetBar">
      <button onclick="loadPreset('llama7b',this)">LLaMA 7B</button>
      <button onclick="loadPreset('llama13b',this)">LLaMA 13B</button>
      <button onclick="loadPreset('llama70b',this)">LLaMA 70B</button>
      <button onclick="loadPreset('mistral7b',this)">Mistral 7B</button>
      <button onclick="loadPreset('mixtral',this)">Mixtral 8x7B</button>
      <button onclick="loadPreset('gpt3',this)">GPT-3 175B</button>
      <button onclick="loadPreset('phi2',this)">Phi-2 2.7B</button>
      <button onclick="loadPreset('qwen72b',this)">Qwen 72B</button>
    </div>

    <div class="section">
      <div class="section-title" data-i18n="sec_arch"></div>

      <div class="field">
        <label><span data-i18n="lbl_params"></span> <span class="hint" id="paramHint">7.0B</span></label>
        <div class="slider-row">
          <input type="range" id="params" min="0.1" max="500" step="0.1" value="7">
          <input type="number" id="paramsNum" min="0.1" max="500" step="0.1" value="7" style="width:5rem">
        </div>
      </div>

      <div class="field">
        <label>Hidden Size <span class="hint" id="hiddenHint">4096</span></label>
        <div class="slider-row">
          <input type="range" id="hidden" min="256" max="16384" step="128" value="4096">
          <input type="number" id="hiddenNum" min="256" max="16384" step="1" value="4096" style="width:5rem">
        </div>
      </div>

      <div class="field">
        <label><span data-i18n="lbl_layers"></span> <span class="hint" id="layerHint">32</span></label>
        <div class="slider-row">
          <input type="range" id="layers" min="1" max="128" step="1" value="32">
          <input type="number" id="layersNum" min="1" max="128" step="1" value="32" style="width:5rem">
        </div>
      </div>

      <div class="field">
        <label><span data-i18n="lbl_heads"></span> <span class="hint" id="headsHint">32</span></label>
        <div class="slider-row">
          <input type="range" id="heads" min="1" max="128" step="1" value="32">
          <input type="number" id="headsNum" min="1" max="128" step="1" value="32" style="width:5rem">
        </div>
      </div>

      <div class="field">
        <label><span data-i18n="lbl_kvheads"></span> <span class="hint" id="kvHeadsHint">32</span></label>
        <div class="slider-row">
          <input type="range" id="kvHeads" min="1" max="128" step="1" value="32">
          <input type="number" id="kvHeadsNum" min="1" max="128" step="1" value="32" style="width:5rem">
        </div>
      </div>

      <div class="field">
        <label data-i18n="lbl_attn_type"></label>
        <select id="attnType" onchange="onAttnChange()">
          <option value="mha">Multi-Head Attention (MHA)</option>
          <option value="gqa">Grouped-Query Attention (GQA)</option>
          <option value="mqa">Multi-Query Attention (MQA)</option>
        </select>
      </div>
    </div>

    <div class="section">
      <div class="section-title" data-i18n="sec_precision"></div>

      <div class="field">
        <label data-i18n="lbl_weight_type"></label>
        <select id="weightType">
          <option value="32">FP32 (32 bit)</option>
          <option value="16" selected>FP16 / BF16 (16 bit)</option>
          <option value="8">FP8 (8 bit)</option>
        </select>
      </div>

      <div class="field">
        <label data-i18n="lbl_quant"></label>
        <select id="quantType">
          <option value="none" data-i18n-opt="quant_none"></option>
          <option value="int8">INT8 (8 bit)</option>
          <option value="int4">INT4 (4 bit)</option>
          <option value="gptq4">GPTQ 4-bit</option>
          <option value="gptq3">GPTQ 3-bit</option>
          <option value="awq4">AWQ 4-bit</option>
          <option value="ggml_q8">GGUF Q8_0</option>
          <option value="ggml_q6">GGUF Q6_K</option>
          <option value="ggml_q5">GGUF Q5_K_M</option>
          <option value="ggml_q4">GGUF Q4_K_M</option>
          <option value="ggml_q3">GGUF Q3_K_M</option>
          <option value="ggml_q2">GGUF Q2_K</option>
        </select>
      </div>
    </div>

    <div class="section">
      <div class="section-title" data-i18n="sec_context"></div>

      <div class="field">
        <label><span data-i18n="lbl_ctx"></span> <span class="hint" id="ctxHint">4 096</span></label>
        <div class="slider-row">
          <input type="range" id="context" min="512" max="131072" step="512" value="4096">
          <input type="number" id="contextNum" min="128" max="131072" step="1" value="4096" style="width:6rem">
        </div>
      </div>

      <div class="field">
        <label>Batch Size <span class="hint" id="batchHint">1</span></label>
        <div class="slider-row">
          <input type="range" id="batch" min="1" max="128" step="1" value="1">
          <input type="number" id="batchNum" min="1" max="128" step="1" value="1" style="width:5rem">
        </div>
      </div>

      <div class="field">
        <label data-i18n="lbl_kv_prec"></label>
        <select id="kvPrec">
          <option value="16" selected>FP16 (16 bit)</option>
          <option value="8">INT8 (8 bit)</option>
          <option value="4">INT4 (4 bit)</option>
        </select>
      </div>
    </div>

    <div class="section">
      <div class="section-title">Multi-GPU</div>

      <div class="field">
        <label><span data-i18n="lbl_gpu_count"></span> <span class="hint" id="gpuCountHint">1</span></label>
        <div class="slider-row">
          <input type="range" id="gpuCount" min="1" max="8" step="1" value="1">
          <input type="number" id="gpuCountNum" min="1" max="8" step="1" value="1" style="width:5rem">
        </div>
      </div>

      <div class="field">
        <label data-i18n="lbl_parallel"></label>
        <select id="parallelism">
          <option value="tensor">Tensor Parallelism</option>
          <option value="pipeline">Pipeline Parallelism</option>
        </select>
      </div>
    </div>

    <div class="section">
      <div class="section-title" data-i18n="sec_misc"></div>

      <div class="field">
        <label><span data-i18n="lbl_overhead"></span> <span class="hint" id="overheadHint">20%</span></label>
        <div class="slider-row">
          <input type="range" id="overhead" min="0" max="50" step="1" value="20">
          <span class="val" id="overheadVal">20%</span>
        </div>
      </div>

      <div class="field">
        <label data-i18n="lbl_mode"></label>
        <select id="mode">
          <option value="inference" data-i18n-opt="mode_inference"></option>
          <option value="training" data-i18n-opt="mode_training"></option>
          <option value="lora" data-i18n-opt="mode_lora"></option>
        </select>
      </div>
    </div>

    <div class="section lora-section" id="loraSection">
      <div class="section-title" data-i18n="sec_lora"></div>

      <div class="field">
        <label>LoRA Rank (r) <span class="hint" id="loraRankHint">16</span></label>
        <div class="slider-row">
          <input type="range" id="loraRank" min="1" max="256" step="1" value="16">
          <input type="number" id="loraRankNum" min="1" max="256" step="1" value="16" style="width:5rem">
        </div>
      </div>

      <div class="field">
        <label data-i18n="lbl_lora_targets"></label>
        <select id="loraTargets">
          <option value="qv" data-i18n-opt="lora_qv"></option>
          <option value="qkv" data-i18n-opt="lora_qkv"></option>
          <option value="qkvo" selected data-i18n-opt="lora_qkvo"></option>
          <option value="all" data-i18n-opt="lora_all"></option>
        </select>
      </div>
    </div>
  </div>

  <!-- Results Panel -->
  <div class="panel results">
    <h2 id="h2Results"></h2>

    <div class="warn-banner" id="warnBanner"></div>

    <div class="info-banner" id="modeBanner"></div>

    <div class="result-card highlight">
      <h3 data-i18n="res_total_vram"></h3>
      <div class="value accent" id="totalVram" aria-live="polite">0 GB</div>
      <div class="sub" id="totalVramSub"></div>
      <div class="multi-gpu-info" id="multiGpuInfo" style="display:none;"></div>
    </div>

    <div class="result-card">
      <h3 data-i18n="res_weights"></h3>
      <div class="value accent2" id="weightMem">0 GB</div>
      <div class="sub" id="weightMemSub"></div>
    </div>

    <div class="result-card">
      <h3>KV-Cache</h3>
      <div class="value accent2" id="kvMem">0 GB</div>
      <div class="sub" id="kvMemSub"></div>
    </div>

    <div class="result-card">
      <h3 data-i18n="res_act_overhead"></h3>
      <div class="value accent2" id="overheadMem">0 GB</div>
    </div>

    <div class="result-card" id="trainingCard" style="display:none;">
      <h3 data-i18n="res_train_extra"></h3>
      <div class="value warn" id="trainMem">0 GB</div>
      <div class="sub" id="trainMemSub"></div>
    </div>

    <div class="result-card">
      <h3 data-i18n="res_breakdown"></h3>
      <div class="breakdown" id="breakdown"></div>
    </div>

    <div class="result-card">
      <h3 data-i18n="res_gpu_rec"></h3>
      <div class="gpu-rec" id="gpuRec"></div>
    </div>
  </div>
</div>

<script>
// ==================== i18n ====================
const I18N = {
  en: {
    headerSub: 'Calculate VRAM and memory requirements for Large Language Models',
    h2Config: 'Model Configuration',
    h2Results: 'Results',
    sec_arch: 'Architecture',
    sec_precision: 'Precision & Quantization',
    sec_context: 'Context & KV-Cache',
    sec_misc: 'Miscellaneous',
    sec_lora: 'LoRA Configuration',
    lbl_params: 'Parameters (Billions)',
    lbl_layers: 'Number of Layers',
    lbl_heads: 'Attention Heads',
    lbl_kvheads: 'KV-Heads',
    lbl_attn_type: 'Attention Type',
    lbl_weight_type: 'Weight Type (Original Model)',
    lbl_quant: 'Quantization',
    lbl_ctx: 'Context Length (Tokens)',
    lbl_kv_prec: 'KV-Cache Precision',
    lbl_gpu_count: 'Number of GPUs',
    lbl_parallel: 'Parallelization',
    lbl_overhead: 'Memory Overhead',
    lbl_mode: 'Mode',
    lbl_lora_targets: 'LoRA Target Modules',
    res_total_vram: 'Total VRAM (GPU)',
    res_weights: 'Model Weights',
    res_act_overhead: 'Activations & Overhead',
    res_train_extra: 'Training Extra Memory',
    res_breakdown: 'Breakdown',
    res_gpu_rec: 'GPU Recommendations',
    quant_none: 'None (Original)',
    mode_inference: 'Inference',
    mode_training: 'Training (Fine-Tuning)',
    mode_lora: 'LoRA Fine-Tuning',
    lora_qv: 'Q + V Projections',
    lora_qkv: 'Q + K + V Projections',
    lora_qkvo: 'Q + K + V + Output Projections',
    lora_all: 'All Linear Layers (incl. FFN)',
    banner_inference: 'Inference mode: Calculates memory for loading and running the model.',
    banner_training: 'Training mode: Includes gradients and optimizer states (Adam). Significantly more memory required.',
    banner_lora: 'LoRA mode: Parameter-efficient fine-tuning. Only adapter weights are trained.',
    warn_headdim: 'Hidden Size ({hidden}) is not divisible by Attention Heads ({heads}). Head dimension will be rounded.',
    bd_weights: 'Model Weights',
    bd_kvcache: 'KV-Cache',
    bd_activations: 'Activations',
    bd_training: 'Training / LoRA',
    bd_overhead: 'Overhead',
    bd_total: 'Total',
    bd_per_gpu: 'Per GPU',
    total_suffix: 'total',
    tp_desc: 'Tensor Parallelism: {mem} per GPU ({n} GPUs)',
    pp_desc: 'Pipeline Parallelism: {mem} per GPU ({n} GPUs, ~{layers} layers/GPU)',
    train_desc_full: 'Master Weights + Gradients + Adam Optimizer (1st/2nd Moment) each FP32',
    train_desc_lora: 'LoRA Adapter (r={rank}, {modules} modules, ~{pct}% of params) + Adam',
    kv_sub: 'KV-Heads: {kvh} ({ratio}% of Q-Heads), {bits} bit',
  },
  de: {
    headerSub: 'Berechne VRAM- und Speicheranforderungen fuer Large Language Models',
    h2Config: 'Modellkonfiguration',
    h2Results: 'Ergebnisse',
    sec_arch: 'Architektur',
    sec_precision: 'Praezision & Quantisierung',
    sec_context: 'Kontext & KV-Cache',
    sec_misc: 'Sonstiges',
    sec_lora: 'LoRA-Konfiguration',
    lbl_params: 'Parameter (Milliarden)',
    lbl_layers: 'Anzahl Layer',
    lbl_heads: 'Attention Heads',
    lbl_kvheads: 'KV-Heads',
    lbl_attn_type: 'Attention-Typ',
    lbl_weight_type: 'Gewichtstyp (Originalmodell)',
    lbl_quant: 'Quantisierung',
    lbl_ctx: 'Kontextlaenge (Tokens)',
    lbl_kv_prec: 'KV-Cache Praezision',
    lbl_gpu_count: 'Anzahl GPUs',
    lbl_parallel: 'Parallelisierung',
    lbl_overhead: 'Speicher-Overhead',
    lbl_mode: 'Modus',
    lbl_lora_targets: 'LoRA Target-Module',
    res_total_vram: 'Gesamt-VRAM (GPU)',
    res_weights: 'Modellgewichte',
    res_act_overhead: 'Aktivierungen & Overhead',
    res_train_extra: 'Training-Zusatzspeicher',
    res_breakdown: 'Aufschluesselung',
    res_gpu_rec: 'GPU-Empfehlungen',
    quant_none: 'Keine (Original)',
    mode_inference: 'Inferenz',
    mode_training: 'Training (Fine-Tuning)',
    mode_lora: 'LoRA Fine-Tuning',
    lora_qv: 'Q + V Projektionen',
    lora_qkv: 'Q + K + V Projektionen',
    lora_qkvo: 'Q + K + V + Output Projektionen',
    lora_all: 'Alle Linear-Layer (inkl. FFN)',
    banner_inference: 'Inferenz-Modus: Berechnet den Speicher fuer das Laden und Ausfuehren des Modells.',
    banner_training: 'Training-Modus: Beinhaltet Gradients und Optimizer-States (Adam). Deutlich mehr Speicher erforderlich.',
    banner_lora: 'LoRA-Modus: Parameter-effizientes Fine-Tuning. Nur Adapter-Gewichte werden trainiert.',
    warn_headdim: 'Hidden Size ({hidden}) ist nicht durch Attention Heads ({heads}) teilbar. Head-Dimension wird gerundet.',
    bd_weights: 'Modellgewichte',
    bd_kvcache: 'KV-Cache',
    bd_activations: 'Aktivierungen',
    bd_training: 'Training / LoRA',
    bd_overhead: 'Overhead',
    bd_total: 'Gesamt',
    bd_per_gpu: 'Pro GPU',
    total_suffix: 'gesamt',
    tp_desc: 'Tensor Parallelism: {mem} pro GPU ({n} GPUs)',
    pp_desc: 'Pipeline Parallelism: {mem} pro GPU ({n} GPUs, ~{layers} Layer/GPU)',
    train_desc_full: 'Master-Weights + Gradients + Adam-Optimizer (1st/2nd Moment) je FP32',
    train_desc_lora: 'LoRA-Adapter (r={rank}, {modules} Module, ~{pct}% der Parameter) + Adam',
    kv_sub: 'KV-Heads: {kvh} ({ratio}% von Q-Heads), {bits} bit',
  }
};

let currentLang = 'en';

function t(key) { return I18N[currentLang][key] || key; }

function setLang(lang) {
  currentLang = lang;
  document.documentElement.lang = lang;
  localStorage.setItem('llm-calc-lang', lang);

  $('langEn').classList.toggle('active', lang === 'en');
  $('langDe').classList.toggle('active', lang === 'de');

  // Update static i18n text nodes
  $('headerSub').textContent = t('headerSub');
  $('h2Config').textContent = t('h2Config');
  $('h2Results').textContent = t('h2Results');

  document.querySelectorAll('[data-i18n]').forEach(el => {
    el.textContent = t(el.getAttribute('data-i18n'));
  });

  // Update select option texts
  document.querySelectorAll('[data-i18n-opt]').forEach(el => {
    el.textContent = t(el.getAttribute('data-i18n-opt'));
  });

  // Re-run calc to update dynamic strings
  calc();
}

// ==================== Constants ====================
const GB_BYTES = 1024 ** 3;
const VRAM_SAFE_THRESHOLD = 0.85;

const PRESETS = {
  llama7b:   { params:7, hidden:4096, layers:32, heads:32, kvHeads:32, attn:'mha', ctx:4096 },
  llama13b:  { params:13, hidden:5120, layers:40, heads:40, kvHeads:40, attn:'mha', ctx:4096 },
  llama70b:  { params:70, hidden:8192, layers:80, heads:64, kvHeads:8, attn:'gqa', ctx:4096 },
  mistral7b: { params:7.2, hidden:4096, layers:32, heads:32, kvHeads:8, attn:'gqa', ctx:32768 },
  mixtral:   { params:46.7, hidden:4096, layers:32, heads:32, kvHeads:8, attn:'gqa', ctx:32768 },
  gpt3:      { params:175, hidden:12288, layers:96, heads:96, kvHeads:96, attn:'mha', ctx:2048 },
  phi2:      { params:2.7, hidden:2560, layers:32, heads:32, kvHeads:32, attn:'mha', ctx:2048 },
  qwen72b:   { params:72, hidden:8192, layers:80, heads:64, kvHeads:8, attn:'gqa', ctx:32768 },
};

const GPUS_SINGLE = [
  { name: 'RTX 3060', vram: 12 },
  { name: 'RTX 4060 Ti', vram: 16 },
  { name: 'RTX 4070 Ti S', vram: 16 },
  { name: 'RTX 4080', vram: 16 },
  { name: 'RTX 3090', vram: 24 },
  { name: 'RTX 4090', vram: 24 },
  { name: 'RTX 5090', vram: 32 },
  { name: 'A100 40GB', vram: 40 },
  { name: 'A100 80GB', vram: 80 },
  { name: 'H100 80GB', vram: 80 },
  { name: 'MI300X', vram: 192 },
];

const QUANT_BPW = {
  none: null,
  int8: 8,
  int4: 4,
  gptq4: 4.25,
  gptq3: 3.25,
  awq4: 4.25,
  ggml_q8: 8.1,
  ggml_q6: 6.6,
  ggml_q5: 5.6,
  ggml_q4: 4.65,
  ggml_q3: 3.6,
  ggml_q2: 2.7,
};

const LORA_TARGET_MODULES = {
  qv: 2,
  qkv: 3,
  qkvo: 4,
  all: 7,
};

// ==================== Helpers ====================
function $(id) { return document.getElementById(id); }

function safeNum(val, fallback, min) {
  const n = parseFloat(val);
  if (isNaN(n) || !isFinite(n)) return fallback;
  if (min !== undefined && n < min) return min;
  return n;
}

function safeInt(val, fallback, min) {
  const n = parseInt(val);
  if (isNaN(n) || !isFinite(n)) return fallback;
  if (min !== undefined && n < min) return min;
  return n;
}

// ==================== Input Sync ====================
let syncing = false;
const pairs = [
  ['params','paramsNum','paramHint', v => v + 'B'],
  ['hidden','hiddenNum','hiddenHint', v => v],
  ['layers','layersNum','layerHint', v => v],
  ['heads','headsNum','headsHint', v => v],
  ['kvHeads','kvHeadsNum','kvHeadsHint', v => v],
  ['context','contextNum','ctxHint', v => Number(v).toLocaleString('de-DE')],
  ['batch','batchNum','batchHint', v => v],
  ['gpuCount','gpuCountNum','gpuCountHint', v => v],
  ['loraRank','loraRankNum','loraRankHint', v => v],
];

pairs.forEach(([slider, num, hint, fmt]) => {
  const s = $(slider), n = $(num), h = $(hint);
  s.addEventListener('input', () => {
    if (syncing) return;
    syncing = true;
    n.value = s.value;
    h.textContent = fmt(s.value);
    syncing = false;
    calc();
  });
  n.addEventListener('input', () => {
    if (syncing) return;
    syncing = true;
    s.value = n.value;
    h.textContent = fmt(n.value);
    syncing = false;
    calc();
  });
});

$('overhead').addEventListener('input', () => {
  $('overheadHint').textContent = $('overhead').value + '%';
  $('overheadVal').textContent = $('overhead').value + '%';
  calc();
});

['weightType','quantType','kvPrec','attnType','mode','parallelism','loraTargets'].forEach(id => {
  $(id).addEventListener('change', calc);
});

$('mode').addEventListener('change', () => {
  $('loraSection').classList.toggle('visible', $('mode').value === 'lora');
});

function onAttnChange() {
  const attn = $('attnType').value;
  const heads = parseInt($('heads').value) || 32;
  if (attn === 'mha') {
    $('kvHeads').value = heads;
    $('kvHeadsNum').value = heads;
    $('kvHeadsHint').textContent = heads;
  } else if (attn === 'mqa') {
    $('kvHeads').value = 1;
    $('kvHeadsNum').value = 1;
    $('kvHeadsHint').textContent = 1;
  }
  calc();
}

function loadPreset(key, btn) {
  document.querySelectorAll('#presetBar button').forEach(b => b.classList.remove('active'));
  if (btn) btn.classList.add('active');

  const p = PRESETS[key];
  $('params').value = p.params; $('paramsNum').value = p.params; $('paramHint').textContent = p.params + 'B';
  $('hidden').value = p.hidden; $('hiddenNum').value = p.hidden; $('hiddenHint').textContent = p.hidden;
  $('layers').value = p.layers; $('layersNum').value = p.layers; $('layerHint').textContent = p.layers;
  $('heads').value = p.heads; $('headsNum').value = p.heads; $('headsHint').textContent = p.heads;
  $('kvHeads').value = p.kvHeads; $('kvHeadsNum').value = p.kvHeads; $('kvHeadsHint').textContent = p.kvHeads;
  $('context').value = p.ctx; $('contextNum').value = p.ctx; $('ctxHint').textContent = Number(p.ctx).toLocaleString('de-DE');
  $('attnType').value = p.attn;
  calc();
}

// ==================== Formatting ====================
function fmt(gb) {
  if (!isFinite(gb) || isNaN(gb)) return 'â€“ GB';
  if (gb < 0.01) return (gb * 1024).toFixed(1) + ' MB';
  if (gb >= 1000) return (gb / 1000).toFixed(2) + ' TB';
  return gb.toFixed(2) + ' GB';
}

// ==================== Main Calculation ====================
function calc() {
  const warnings = [];

  const params    = safeNum($('paramsNum').value, 7, 0.1) * 1e9;
  const hidden    = safeInt($('hiddenNum').value, 4096, 64);
  const layers    = safeInt($('layersNum').value, 32, 1);
  const nHeads    = safeInt($('headsNum').value, 32, 1);
  const nKvHeads  = safeInt($('kvHeadsNum').value, 32, 1);
  const weightBits = parseInt($('weightType').value);
  const quantType = $('quantType').value;
  const ctxLen    = safeInt($('contextNum').value, 4096, 1);
  const batchSize = safeInt($('batchNum').value, 1, 1);
  const kvBits    = parseInt($('kvPrec').value);
  const overheadPct = safeInt($('overhead').value, 20, 0) / 100;
  const mode      = $('mode').value;
  const numGpus   = safeInt($('gpuCountNum').value, 1, 1);
  const parallel  = $('parallelism').value;
  const loraRank  = safeInt($('loraRankNum').value, 16, 1);
  const loraTargetKey = $('loraTargets').value;

  if (hidden % nHeads !== 0) {
    warnings.push(t('warn_headdim').replace('{hidden}', hidden).replace('{heads}', nHeads));
  }
  const headDim = Math.floor(hidden / nHeads);

  let effectiveBpw = (quantType === 'none') ? weightBits : QUANT_BPW[quantType];

  // Model weights
  const weightGB = (params * (effectiveBpw / 8)) / GB_BYTES;

  // KV-Cache
  const kvCacheGB = (2 * layers * nKvHeads * headDim * ctxLen * batchSize * (kvBits / 8)) / GB_BYTES;

  // Activations
  let activationBytes;
  if (mode === 'inference') {
    activationBytes = batchSize * ctxLen * hidden * 4 * 2;
  } else if (mode === 'training') {
    const seqFactor = 34 + (5 * nHeads * ctxLen / hidden);
    activationBytes = layers * batchSize * ctxLen * hidden * seqFactor;
  } else {
    const seqFactor = 34 + (5 * nHeads * ctxLen / hidden);
    activationBytes = layers * batchSize * ctxLen * hidden * seqFactor * 0.5;
  }
  const activationGB = activationBytes / GB_BYTES;

  // Training overhead
  let trainGB = 0;
  let trainDesc = '';
  if (mode === 'training') {
    trainGB = (params * 16) / GB_BYTES;
    trainDesc = t('train_desc_full');
  } else if (mode === 'lora') {
    const numTargets = LORA_TARGET_MODULES[loraTargetKey];
    const loraParams = 2 * loraRank * hidden * numTargets * layers;
    const loraParamsPct = ((loraParams / params) * 100).toFixed(2);
    trainGB = (loraParams * 16) / GB_BYTES;
    trainDesc = t('train_desc_lora')
      .replace('{rank}', loraRank)
      .replace('{modules}', numTargets)
      .replace('{pct}', loraParamsPct);
  }

  // Overhead (not on optimizer states)
  const inferenceBase = weightGB + kvCacheGB + activationGB;
  const overheadGB = inferenceBase * overheadPct;
  const totalGB = inferenceBase + trainGB + overheadGB;

  // Multi-GPU
  let perGpuGB;
  let multiGpuDesc = '';
  if (numGpus > 1) {
    if (parallel === 'tensor') {
      const perGpuWeights = weightGB / numGpus;
      const perGpuKv = kvCacheGB / numGpus;
      const perGpuAct = activationGB / numGpus;
      const perGpuTrain = trainGB / numGpus;
      const perGpuOh = (perGpuWeights + perGpuKv + perGpuAct) * overheadPct;
      perGpuGB = perGpuWeights + perGpuKv + perGpuAct + perGpuTrain + perGpuOh;
      multiGpuDesc = t('tp_desc').replace('{mem}', fmt(perGpuGB)).replace('{n}', numGpus);
    } else {
      const layersPerGpu = Math.ceil(layers / numGpus);
      const perGpuWeights = weightGB / numGpus;
      const perGpuKv = kvCacheGB / numGpus;
      const perGpuAct = activationGB;
      const perGpuTrain = trainGB / numGpus;
      const perGpuOh = (perGpuWeights + perGpuKv + perGpuAct) * overheadPct;
      perGpuGB = perGpuWeights + perGpuKv + perGpuAct + perGpuTrain + perGpuOh;
      multiGpuDesc = t('pp_desc').replace('{mem}', fmt(perGpuGB)).replace('{n}', numGpus).replace('{layers}', layersPerGpu);
    }
  } else {
    perGpuGB = totalGB;
  }

  // === Update UI ===
  $('totalVram').textContent = fmt(totalGB);
  $('totalVramSub').textContent = `${(totalGB * 1024).toFixed(0)} MB ${t('total_suffix')}`;

  const multiInfo = $('multiGpuInfo');
  if (numGpus > 1) {
    multiInfo.style.display = 'block';
    multiInfo.textContent = multiGpuDesc;
  } else {
    multiInfo.style.display = 'none';
  }

  $('weightMem').textContent = fmt(weightGB);
  $('weightMemSub').textContent = `${effectiveBpw.toFixed(1)} bit/weight`;
  $('kvMem').textContent = fmt(kvCacheGB);

  const kvRatio = nKvHeads / nHeads;
  $('kvMemSub').textContent = t('kv_sub')
    .replace('{kvh}', nKvHeads)
    .replace('{ratio}', (kvRatio * 100).toFixed(0))
    .replace('{bits}', kvBits);

  $('overheadMem').textContent = fmt(activationGB + overheadGB);

  const trainingCard = $('trainingCard');
  if (mode === 'inference') {
    trainingCard.style.display = 'none';
    $('modeBanner').textContent = t('banner_inference');
  } else {
    trainingCard.style.display = 'block';
    $('trainMem').textContent = fmt(trainGB);
    $('trainMemSub').textContent = trainDesc;
    $('modeBanner').textContent = mode === 'training' ? t('banner_training') : t('banner_lora');
  }

  // Warnings
  const warnBanner = $('warnBanner');
  if (warnings.length > 0) {
    warnBanner.style.display = 'block';
    warnBanner.innerHTML = warnings.join('<br>');
  } else {
    warnBanner.style.display = 'none';
  }

  // Breakdown
  let html = `
    <div class="breakdown-row"><span class="label">${t('bd_weights')}</span><span class="val">${fmt(weightGB)}</span></div>
    <div class="breakdown-row"><span class="label">${t('bd_kvcache')}</span><span class="val">${fmt(kvCacheGB)}</span></div>
    <div class="breakdown-row"><span class="label">${t('bd_activations')}</span><span class="val">${fmt(activationGB)}</span></div>`;
  if (trainGB > 0) {
    html += `<div class="breakdown-row"><span class="label">${t('bd_training')}</span><span class="val">${fmt(trainGB)}</span></div>`;
  }
  html += `
    <div class="breakdown-row"><span class="label">${t('bd_overhead')} (${(overheadPct*100).toFixed(0)}%)</span><span class="val">${fmt(overheadGB)}</span></div>
    <div class="breakdown-row" style="border-top:2px solid var(--accent);padding-top:0.6rem;font-weight:700;">
      <span class="label">${t('bd_total')}</span><span class="val" style="color:var(--accent)">${fmt(totalGB)}</span>
    </div>`;
  if (numGpus > 1) {
    html += `
    <div class="breakdown-row" style="border-top:1px dashed var(--accent2);padding-top:0.5rem;">
      <span class="label">${t('bd_per_gpu')} (${numGpus}x)</span><span class="val" style="color:var(--accent2)">${fmt(perGpuGB)}</span>
    </div>`;
  }
  $('breakdown').innerHTML = html;

  // GPU recommendations
  const rec = $('gpuRec');
  rec.innerHTML = '';
  GPUS_SINGLE.forEach(gpu => {
    const tag = document.createElement('span');
    tag.className = 'gpu-tag';
    const label = numGpus > 1 ? `${numGpus}x ${gpu.name}` : gpu.name + ` ${gpu.vram}GB`;
    if (perGpuGB <= gpu.vram * VRAM_SAFE_THRESHOLD) {
      tag.classList.add('green');
      tag.textContent = label + ' \u2714';
    } else if (perGpuGB <= gpu.vram) {
      tag.classList.add('yellow');
      tag.textContent = label + ' \u26A0';
    } else {
      tag.classList.add('red');
      tag.textContent = label + ' \u2718';
    }
    rec.appendChild(tag);
  });
}

// ==================== Init ====================
const savedLang = localStorage.getItem('llm-calc-lang') || 'en';
setLang(savedLang);
</script>

</body>
</html>
